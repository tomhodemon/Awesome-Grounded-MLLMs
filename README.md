# Awesome-Grounded-MLLMs

## Table of Contents
- [Grounded MLLMs](#grounded-mllms)
    - [Grounded Chain-of-Thought](#grounded-chain-of-thought)
    - [Grounded Chat](#grounded-chat) 
    - [General Purpose](#general-purpose)

## Grounded MLLMs

### Grounded Chain-of-Thought
|  Title  |   Venue  |   Date   | Supplement|
|:--------|:--------:|:--------:|:--------:|
| [Grounded Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2503.12799) | Arxiv | 2025-03-17 | [Code](https://anonymous.4open.science/r/GCoT-04D1/README.md) 


### Grounded Chat
|  Title  |   Venue  |   Date   | Supplement|
|:--------|:--------:|:--------:|:--------:|
| [Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01023.pdf) | ECCV2024 | 2024-04-19 | [Project Page](https://groma-mllm.github.io/)


### General Purpose
|  Title  |   Venue  |   Date   | Supplement|
|:--------|:--------:|:--------:|:--------:|
| [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.pdf) | CVPR2024 |Â 2024-6-16 | 